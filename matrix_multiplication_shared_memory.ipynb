{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M5VyaH-qI0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0503a055-4b9b-4ab9-ddc7-71cad428c4aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (2024.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda) (2023.1.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modd = SourceModule(\"\"\"\n",
        "#define TILE_DIM 32\n",
        "\n",
        "__global__ void matrix_multiplication_shared_memory(const double* A, const double* B, double* C, int N, int M, int K)\n",
        "{\n",
        "\n",
        "\t  // Allocate the sub-matrices to the shared memory. Note two-dim indexing.\n",
        "    __shared__ double sub_A[TILE_DIM][TILE_DIM];\n",
        "    __shared__ double sub_B[TILE_DIM][TILE_DIM];\n",
        "\n",
        "    const int tx = threadIdx.x;\n",
        "    const int ty = threadIdx.y;\n",
        "\n",
        "    const int bx = blockIdx.x;\n",
        "    const int by = blockIdx.y;\n",
        "\n",
        "    // Each block gets it own TILE_DIM sized slot in x and y directions.\n",
        "    const int row = by * TILE_DIM + ty;\n",
        "    const int col = bx * TILE_DIM + tx;\n",
        "\n",
        "    double result = 0.0;\n",
        "\n",
        "    for(int i = 0; i <  M  / TILE_DIM; i++)\n",
        "        {\n",
        "\n",
        "        // Iterate over the tile dimension to copy the data.\n",
        "        sub_A[ty][tx] = A[(i * TILE_DIM + tx) + M * row];\n",
        "        sub_B[ty][tx] = B[(i * TILE_DIM + ty) * K + col];\n",
        "\n",
        "\t\t    // Make sure that all threads have completed the memory transaction.\n",
        "        __syncthreads();\n",
        "\n",
        "        // Multiply the matrix elements inside the tile and add them to the result.\n",
        "        for (int j = 0; j < TILE_DIM; j++)\n",
        "            {\n",
        "            result += sub_A[ty][j] * sub_B[j][tx];\n",
        "            }\n",
        "\n",
        "\t\t    // Make sure that all of the threads have finished the calculation\n",
        "        __syncthreads();\n",
        "        }\n",
        "\n",
        "    // Write back to the global memory. Using the global index.\n",
        "    int C_index = K * (by * blockDim.y + ty) + (bx * blockDim.x + tx);\n",
        "    C[C_index] = result;\n",
        "}\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "5mZW0etV3hl9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up tests.\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "A_n_rows = 2048\n",
        "A_n_cols = 1024\n",
        "B_n_rows = 1024\n",
        "B_n_cols = 512\n",
        "\n",
        "value_type = float\n",
        "\n",
        "C_n_rows = A_n_rows\n",
        "C_n_cols = B_n_cols\n",
        "\n",
        "numThreadsPerBlock = 32\n",
        "numBlocksx = math.ceil(C_n_cols/numThreadsPerBlock)\n",
        "numBlocksy = math.ceil(C_n_rows/numThreadsPerBlock)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-unKjbaZy54B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the input vectors.\n",
        "\n",
        "A = np.random.randn(A_n_rows,A_n_cols)\n",
        "A = A.astype(value_type)\n",
        "\n",
        "B = np.random.randn(B_n_rows,B_n_cols)\n",
        "B = B.astype(value_type)\n",
        "\n",
        "\n",
        "# Allocate the memory on the GPU and copy the vectors.\n",
        "\n",
        "A_gpu = cuda.mem_alloc(A.size * A.dtype.itemsize)\n",
        "cuda.memcpy_htod(A_gpu, A)\n",
        "\n",
        "B_gpu = cuda.mem_alloc(B.size * B.dtype.itemsize)\n",
        "cuda.memcpy_htod(B_gpu, B)\n",
        "\n",
        "C = np.zeros([C_n_rows,C_n_cols])\n",
        "C = C.astype(value_type)\n",
        "C_gpu = cuda.mem_alloc(C.size * C.dtype.itemsize)\n",
        "\n",
        "\n",
        " # Call the CUDA kernel.\n",
        "\n",
        "matrix_multiplication = modd.get_function(\"matrix_multiplication_shared_memory\")\n",
        "matrix_multiplication(A_gpu, B_gpu, C_gpu, np.int32(A_n_rows), np.int32(A_n_cols), np.int32(B_n_cols),\n",
        "                        block=(numThreadsPerBlock, numThreadsPerBlock, 1),\n",
        "                        grid=(numBlocksx, numBlocksy, 1))\n",
        "\n",
        "\n",
        "# Copy the result back to the host.\n",
        "\n",
        "cuda.memcpy_dtoh(C, C_gpu)\n",
        "\n",
        "# Do same calculation in CPU.\n",
        "\n",
        "C_cpu = np.dot(A,B)\n",
        "\n",
        "# Verify the result\n",
        "\n",
        "np.allclose(C_cpu,C, 0.001,0.001)\n"
      ],
      "metadata": {
        "id": "L1Ea2UiC8EtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8b0d77-d94b-4980-d8e0-1c5a12ba9995"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n",
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}